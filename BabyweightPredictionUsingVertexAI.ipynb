{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Structured data prediction using Vertex AI Platform\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Create a BigQuery Dataset and Google Cloud Storage Bucket \n",
    "2. Export from BigQuery to CSVs in GCS\n",
    "3. Training on Cloud AI Platform\n",
    "4. Deploy trained model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you train, evaluate, and deploy a machine learning model to predict a baby's weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nny3m465gKkY"
   },
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user google-cloud-bigquery>=2.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.120.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.124.0-py2.py3-none-any.whl.metadata (45 kB)\n",
      "Requirement already satisfied: shapely>=2 in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.41.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (6.31.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (25.0)\n",
      "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.38.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.14.2)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.42.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.12.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.15.0)\n",
      "Requirement already satisfied: docstring_parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from shapely>=2) (1.26.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.11.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
      "Downloading google_cloud_aiplatform-1.124.0-py2.py3-none-any.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.120.0\n",
      "    Uninstalling google-cloud-aiplatform-1.120.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.120.0\n",
      "Successfully installed google-cloud-aiplatform-1.124.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U google-cloud-aiplatform \"shapely>=2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart your kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore the deprecation warnings and incompatibility errors related to google-cloud-storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJ7ByvoXzpVI"
   },
   "source": [
    "## Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set environment variables so that we can use them throughout the entire notebook. We will be using our project name for our bucket, so you only need to change your project and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'testbucket1_unique' # Replace with the your bucket name\n",
    "PROJECT = 'qwiklabs-gcp-03-652967eb1d9b' # Replace with your project-id\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.11\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: qwiklabs-gcp-03-652967eb1d9b\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"$PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0-vOB4y2BJM"
   },
   "source": [
    "## The source dataset\n",
    "\n",
    "Our dataset is hosted in [BigQuery](https://cloud.google.com/bigquery/). The CDC's Natality data has details on US births from 1969 to 2008 and is a publically available dataset, meaning anyone with a GCP account has access. Click [here](https://console.cloud.google.com/bigquery?project=bigquery-public-data&p=publicdata&d=samples&t=natality&page=table) to access the dataset.\n",
    "\n",
    "The natality dataset is relatively large at almost 138 million rows and 31 columns, but simple to understand. `weight_pounds` is the target, the continuous value we’ll train a model to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BigQuery Dataset and Google Cloud Storage Bucket \n",
    "\n",
    "A BigQuery dataset is a container for tables, views, and models built with BigQuery ML. Let's create one called __babyweight__. We'll do the same for a GCS bucket for our project too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BigQuery dataset titled: babyweight\n",
      "Dataset 'qwiklabs-gcp-03-652967eb1d9b:babyweight' successfully created.\n",
      "Here are your current datasets:\n",
      "  datasetId   \n",
      " ------------ \n",
      "  babyweight  \n",
      "Bucket exists, let's not recreate it.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create a BigQuery dataset for babyweight if it doesn't exist\n",
    "datasetexists=$(bq ls -d | grep -w babyweight)\n",
    "\n",
    "if [ -n \"$datasetexists\" ]; then\n",
    "    echo -e \"BigQuery dataset already exists, let's not recreate it.\"\n",
    "\n",
    "else\n",
    "    echo \"Creating BigQuery dataset titled: babyweight\"\n",
    "    \n",
    "    bq --location=US mk --dataset \\\n",
    "        --description \"Babyweight\" \\\n",
    "        $PROJECT:babyweight\n",
    "    echo \"Here are your current datasets:\"\n",
    "    bq ls\n",
    "fi\n",
    "    \n",
    "## Create GCS bucket if it doesn't exist already...\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "    echo -e \"Bucket exists, let's not recreate it.\"\n",
    "    \n",
    "else\n",
    "    echo \"Creating a new GCS bucket.\"\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    echo \"Here are your current buckets:\"\n",
    "    gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2TuS1s9vREL"
   },
   "source": [
    "## Create the training and evaluation data tables\n",
    "\n",
    "Since there is already a publicly available dataset, we can simply create the training and evaluation data tables using this raw input data. First we are going to create a subset of the data limiting our columns to `weight_pounds`, `is_male`, `mother_age`, `plurality`, and `gestation_weeks` as well as some simple filtering and a column to hash on for repeatable splitting.\n",
    "\n",
    "* Note:  The dataset in the create table code below is the one created previously, e.g. \"babyweight\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and filter dataset\n",
    "\n",
    "We have some preprocessing and filtering we would like to do to get our data in the right format for training.\n",
    "\n",
    "Preprocessing:\n",
    "* Cast `is_male` from `BOOL` to `STRING`\n",
    "* Cast `plurality` from `INTEGER` to `STRING` where `[1, 2, 3, 4, 5]` becomes `[\"Single(1)\", \"Twins(2)\", \"Triplets(3)\", \"Quadruplets(4)\", \"Quintuplets(5)\"]`\n",
    "* Add `hashcolumn` hashing on `year` and `month`\n",
    "\n",
    "Filtering:\n",
    "* Only want data for years later than `2000`\n",
    "* Only want baby weights greater than `0`\n",
    "* Only want mothers whose age is greater than `0`\n",
    "* Only want plurality to be greater than `0`\n",
    "* Only want the number of weeks of gestation to be greater than `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0d159efda248768c4cba08a24bbf64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    CAST(is_male AS STRING) AS is_male,\n",
    "    mother_age,\n",
    "    CASE\n",
    "        WHEN plurality = 1 THEN \"Single(1)\"\n",
    "        WHEN plurality = 2 THEN \"Twins(2)\"\n",
    "        WHEN plurality = 3 THEN \"Triplets(3)\"\n",
    "        WHEN plurality = 4 THEN \"Quadruplets(4)\"\n",
    "        WHEN plurality = 5 THEN \"Quintuplets(5)\"\n",
    "    END AS plurality,\n",
    "    gestation_weeks,\n",
    "    FARM_FINGERPRINT(\n",
    "        CONCAT(\n",
    "            CAST(year AS STRING),\n",
    "            CAST(month AS STRING)\n",
    "        )\n",
    "    ) AS hashmonth\n",
    "FROM\n",
    "    publicdata.samples.natality\n",
    "WHERE\n",
    "    year > 2000\n",
    "    AND weight_pounds > 0\n",
    "    AND mother_age > 0\n",
    "    AND plurality > 0\n",
    "    AND gestation_weeks > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment dataset to simulate missing data\n",
    "\n",
    "Now we want to augment our dataset with our simulated babyweight data by setting all gender information to `Unknown` and setting plurality of all non-single births to `Multiple(2+)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0679a836ae4b3391936c44cdf27c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_augmented_data AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks,\n",
    "    hashmonth\n",
    "FROM\n",
    "    babyweight.babyweight_data\n",
    "UNION ALL\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    \"Unknown\" AS is_male,\n",
    "    mother_age,\n",
    "    CASE\n",
    "        WHEN plurality = \"Single(1)\" THEN plurality\n",
    "        ELSE \"Multiple(2+)\"\n",
    "    END AS plurality,\n",
    "    gestation_weeks,\n",
    "    hashmonth\n",
    "FROM\n",
    "    babyweight.babyweight_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split augmented dataset into train and eval sets\n",
    "\n",
    "Using `hashmonth`, apply a module to get approximately a 75/25 train-eval split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split augmented dataset into train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMNRractvREL"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab324efc677456db342b218a522f631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data_train AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks\n",
    "FROM\n",
    "    babyweight.babyweight_augmented_data\n",
    "WHERE\n",
    "    ABS(MOD(hashmonth, 4)) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split augmented dataset into eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc6d88ca23a4d51b4abbe70f2dfbfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data_eval AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks\n",
    "FROM\n",
    "    babyweight.babyweight_augmented_data\n",
    "WHERE\n",
    "    ABS(MOD(hashmonth, 4)) = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clnaaqQsXkwC"
   },
   "source": [
    "## Verify table creation\n",
    "\n",
    "Verify that you created the dataset and training data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace66f73112b4de4bf5aee167b06b485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3d7cba70064191a22c7b46df0fb941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: |          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_pounds</th>\n",
       "      <th>is_male</th>\n",
       "      <th>mother_age</th>\n",
       "      <th>plurality</th>\n",
       "      <th>gestation_weeks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [weight_pounds, is_male, mother_age, plurality, gestation_weeks]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "-- LIMIT 0 is a free query; this allows us to check that the table exists.\n",
    "SELECT * FROM babyweight.babyweight_data_train\n",
    "LIMIT 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7e2cb116f54095b5b1f658c0855dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b54bd4b0fc147e2bcbbe5762b732b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: |          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_pounds</th>\n",
       "      <th>is_male</th>\n",
       "      <th>mother_age</th>\n",
       "      <th>plurality</th>\n",
       "      <th>gestation_weeks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [weight_pounds, is_male, mother_age, plurality, gestation_weeks]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "-- LIMIT 0 is a free query; this allows us to check that the table exists.\n",
    "SELECT * FROM babyweight.babyweight_data_eval\n",
    "LIMIT 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export from BigQuery to CSVs in GCS\n",
    "\n",
    "Use BigQuery Python API to export our train and eval tables to Google Cloud Storage in the CSV format to be used later for TensorFlow/Keras training. We'll want to use the dataset we've been using above as well as repeat the process for both training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported qwiklabs-gcp-03-652967eb1d9b:babyweight.babyweight_data_train to gs://testbucket1_unique/babyweight/data/train*.csv\n",
      "Exported qwiklabs-gcp-03-652967eb1d9b:babyweight.babyweight_data_eval to gs://testbucket1_unique/babyweight/data/eval*.csv\n"
     ]
    }
   ],
   "source": [
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "dataset_name = \"babyweight\"\n",
    "\n",
    "# Create dataset reference object\n",
    "dataset_ref = client.dataset(\n",
    "    dataset_id=dataset_name, project=client.project)\n",
    "\n",
    "# Export both train and eval tables\n",
    "for step in [\"train\", \"eval\"]:\n",
    "    destination_uri = os.path.join(\n",
    "        \"gs://\", BUCKET, dataset_name, \"data\", \"{}*.csv\".format(step))\n",
    "    table_name = \"babyweight_data_{}\".format(step)\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref,\n",
    "        destination_uri,\n",
    "        # Location must match that of the source table.\n",
    "        location=\"US\",\n",
    "    )  # API request\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "    print(\"Exported {}:{}.{} to {}\".format(\n",
    "        client.project, dataset_name, table_name, destination_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify CSV creation\n",
    "\n",
    "Verify that we correctly created the CSV files in our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://testbucket1_unique/babyweight/data/eval000000000000.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000001.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000002.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000003.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000004.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000005.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000006.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000007.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000008.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000009.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000010.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000011.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000012.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000013.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000014.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000015.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000016.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000017.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000018.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000019.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000020.csv\n",
      "gs://testbucket1_unique/babyweight/data/eval000000000021.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000000.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000001.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000002.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000003.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000004.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000005.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000006.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000007.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000008.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000009.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000010.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000011.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000012.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000013.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000014.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000015.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000016.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000017.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000018.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000019.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000020.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000021.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000022.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000023.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000024.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000025.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000026.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000027.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000028.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000029.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000030.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000031.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000032.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000033.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000034.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000035.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000036.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000037.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000038.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000039.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000040.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000041.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000042.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000043.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000044.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000045.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000046.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000047.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000048.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000049.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000050.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000051.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000052.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000053.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000054.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000055.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000056.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000057.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000058.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000059.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000060.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000061.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000062.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000063.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000064.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000065.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000066.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000067.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000068.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000069.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000070.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000071.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000072.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000073.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000074.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000075.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000076.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000077.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000078.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000079.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000080.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000081.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000082.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000083.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000084.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data exists\n",
    "\n",
    "Verify that you previously created CSV files we'll be using for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://testbucket1_unique/babyweight/data/eval000000000000.csv\n",
      "gs://testbucket1_unique/babyweight/data/train000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/data/*000000000000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.12/subprocess.py:1010: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.12/subprocess.py:1016: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 664B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
      "#2 DONE 0.3s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [internal] load build context\n",
      "#4 transferring context: 14.50kB done\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [1/7] FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest@sha256:07e7ea696bf78f9e51c481f74c4cee100c00870cad74b04de849aea7e66f8c51\n",
      "#5 resolve us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest@sha256:07e7ea696bf78f9e51c481f74c4cee100c00870cad74b04de849aea7e66f8c51 0.0s done\n",
      "#5 sha256:2c270b121e8185d98ca3a832f61981f1a8529ef7d14e105550e06bb11c649f44 12.93kB / 12.93kB done\n",
      "#5 sha256:57c139bbda7eb92a286d974aa8fef81acf1a8cbc742242619252c13b196ab499 1.05MB / 29.55MB 0.1s\n",
      "#5 sha256:6289371d2bb385802456c41320c8f9d1f2d2c92345a822581eb3709c500a7ade 0B / 327B 0.1s\n",
      "#5 sha256:b36d8262105cab4f33595944b496f3da007439e3d20d2c4216162715839d492a 0B / 3.60kB 0.1s\n",
      "#5 sha256:07e7ea696bf78f9e51c481f74c4cee100c00870cad74b04de849aea7e66f8c51 5.75kB / 5.75kB done\n",
      "#5 sha256:57c139bbda7eb92a286d974aa8fef81acf1a8cbc742242619252c13b196ab499 29.55MB / 29.55MB 0.3s\n",
      "#5 sha256:6289371d2bb385802456c41320c8f9d1f2d2c92345a822581eb3709c500a7ade 327B / 327B 0.1s done\n",
      "#5 sha256:b36d8262105cab4f33595944b496f3da007439e3d20d2c4216162715839d492a 3.60kB / 3.60kB 0.1s done\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 11.53MB / 102.59MB 0.3s\n",
      "#5 sha256:275a246fad1f928208d5113b37f73e58516d6acaad2365791187591074dda3ba 1.44kB / 1.44kB 0.3s done\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 0B / 202.20MB 0.3s\n",
      "#5 sha256:57c139bbda7eb92a286d974aa8fef81acf1a8cbc742242619252c13b196ab499 29.55MB / 29.55MB 0.3s done\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 30.41MB / 102.59MB 0.5s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 14.68MB / 202.20MB 0.5s\n",
      "#5 extracting sha256:57c139bbda7eb92a286d974aa8fef81acf1a8cbc742242619252c13b196ab499\n",
      "#5 sha256:bbce0e9b489fb7d2d774c017c9d66b4ddc3f21711b4ab515cd7476ecb23108f9 5.67MB / 5.67MB 0.5s done\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 26.21MB / 202.20MB 0.6s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 0B / 168.79MB 0.6s\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 45.09MB / 102.59MB 0.7s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 36.70MB / 202.20MB 0.7s\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 53.48MB / 102.59MB 0.8s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 48.00MB / 202.20MB 0.8s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 16.78MB / 168.79MB 0.8s\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 74.45MB / 102.59MB 1.0s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 59.25MB / 202.20MB 0.9s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 36.70MB / 168.79MB 1.0s\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 84.93MB / 102.59MB 1.1s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 78.64MB / 202.20MB 1.1s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 49.28MB / 168.79MB 1.1s\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 95.42MB / 102.59MB 1.2s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 60.82MB / 168.79MB 1.2s\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 102.59MB / 102.59MB 1.3s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 98.57MB / 202.20MB 1.3s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 71.86MB / 168.79MB 1.3s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 82.21MB / 168.79MB 1.4s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 119.23MB / 202.20MB 1.5s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 94.09MB / 168.79MB 1.5s\n",
      "#5 sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 102.59MB / 102.59MB 1.7s done\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 141.56MB / 202.20MB 1.7s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 116.39MB / 168.79MB 1.7s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 0B / 853.67MB 1.7s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 162.53MB / 202.20MB 1.9s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 137.36MB / 168.79MB 1.9s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 146.80MB / 168.79MB 2.0s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 184.55MB / 202.20MB 2.1s\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 202.20MB / 202.20MB 2.3s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 168.79MB / 168.79MB 2.3s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 49.28MB / 853.67MB 2.3s\n",
      "#5 extracting sha256:57c139bbda7eb92a286d974aa8fef81acf1a8cbc742242619252c13b196ab499 2.3s done\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 112.20MB / 853.67MB 2.8s\n",
      "#5 sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 168.79MB / 168.79MB 2.9s done\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 159.38MB / 853.67MB 3.2s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 206.57MB / 853.67MB 3.6s\n",
      "#5 extracting sha256:b36d8262105cab4f33595944b496f3da007439e3d20d2c4216162715839d492a\n",
      "#5 sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 202.20MB / 202.20MB 3.9s done\n",
      "#5 extracting sha256:b36d8262105cab4f33595944b496f3da007439e3d20d2c4216162715839d492a done\n",
      "#5 extracting sha256:6289371d2bb385802456c41320c8f9d1f2d2c92345a822581eb3709c500a7ade done\n",
      "#5 sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 32B / 32B 4.0s done\n",
      "#5 sha256:e4bff5ce614c6a0cd7b3861e686252c65185413921af4563139cafdbf798edcd 0B / 128B 4.0s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 257.95MB / 853.67MB 4.1s\n",
      "#5 sha256:e4bff5ce614c6a0cd7b3861e686252c65185413921af4563139cafdbf798edcd 128B / 128B 4.0s done\n",
      "#5 extracting sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555\n",
      "#5 sha256:2999545bd1924d977146cd119ab0fafa3fb5848a9a43d87b81c1079adfec3737 899B / 899B 4.1s done\n",
      "#5 sha256:bd33b5a5ec4a1375956c0d91d660e25482f5d75242e465699d5306d43627c403 0B / 445B 4.1s\n",
      "#5 sha256:bd33b5a5ec4a1375956c0d91d660e25482f5d75242e465699d5306d43627c403 445B / 445B 4.1s done\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 0B / 253.99MB 4.2s\n",
      "#5 sha256:a73bd9480671f5b4554219bcdc9f743d568dafb25a91037344681cbc4a3a1e80 0B / 691B 4.2s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 15.41MB / 253.99MB 4.4s\n",
      "#5 sha256:a73bd9480671f5b4554219bcdc9f743d568dafb25a91037344681cbc4a3a1e80 691B / 691B 4.3s done\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 0B / 534.48MB 4.4s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 303.54MB / 853.67MB 4.6s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 34.57MB / 253.99MB 4.6s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 32.51MB / 534.48MB 4.8s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 54.53MB / 253.99MB 4.9s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 71.30MB / 253.99MB 5.1s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 350.22MB / 853.67MB 5.2s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 62.91MB / 534.48MB 5.2s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 88.08MB / 253.99MB 5.3s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 106.95MB / 253.99MB 5.5s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 96.60MB / 534.48MB 5.6s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 123.73MB / 253.99MB 5.7s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 393.22MB / 853.67MB 5.8s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 144.70MB / 253.99MB 6.0s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 125.31MB / 534.48MB 6.0s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 162.53MB / 253.99MB 6.2s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 437.26MB / 853.67MB 6.4s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 180.36MB / 253.99MB 6.4s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 166.85MB / 534.48MB 6.6s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 200.28MB / 253.99MB 6.7s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 484.44MB / 853.67MB 7.0s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 228.59MB / 253.99MB 7.0s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 198.18MB / 534.48MB 7.0s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 243.27MB / 253.99MB 7.2s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 228.59MB / 534.48MB 7.4s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 532.68MB / 853.67MB 7.7s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 261.49MB / 534.48MB 7.8s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 577.77MB / 853.67MB 8.2s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 295.70MB / 534.48MB 8.2s\n",
      "#5 sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 253.99MB / 253.99MB 8.5s done\n",
      "#5 sha256:d7df78d5fe6174c6b7690274bbcdaae9eba2775740ec950b2757e397b1041ea7 0B / 176B 8.5s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 324.01MB / 534.48MB 8.6s\n",
      "#5 sha256:d7df78d5fe6174c6b7690274bbcdaae9eba2775740ec950b2757e397b1041ea7 176B / 176B 8.6s done\n",
      "#5 sha256:7daa1f38b6ab7e6d39825bf159fdbcf5f8cef0eac8e02c0dc3b05defcba49db7 156B / 156B 8.7s done\n",
      "#5 sha256:bce9940ec07e9ed16d28173ba85cb9d158516b0cedc387b30a0285063aedc025 0B / 4.67kB 8.8s\n",
      "#5 sha256:bce9940ec07e9ed16d28173ba85cb9d158516b0cedc387b30a0285063aedc025 4.67kB / 4.67kB 8.8s done\n",
      "#5 sha256:01f79f64a163d3fa3dcd8d388b0037649ed536eb50e2c3234c46bf4ead66c527 0B / 154B 8.9s\n",
      "#5 extracting sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 5.1s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 628.10MB / 853.67MB 9.2s\n",
      "#5 sha256:01f79f64a163d3fa3dcd8d388b0037649ed536eb50e2c3234c46bf4ead66c527 154B / 154B 9.0s done\n",
      "#5 sha256:5113ced62272a5ed4d9dbc29b9d27b24e83c9959a9d4f8dce4debf495cc4a26d 163B / 163B 9.2s done\n",
      "#5 sha256:aa0eae780ea1755c84561d2e50a1f8d79ad5dd51f0b8f5f41287cb249ae3b8ed 11.72kB / 11.72kB 9.3s done\n",
      "#5 sha256:caeb9ac3c4842526e1d3737860f415263105d1a7e8fb2d35bdad31d829aede1a 1.38kB / 1.38kB 9.4s done\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 357.56MB / 534.48MB 9.7s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 672.40MB / 853.67MB 10.2s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 386.49MB / 534.48MB 10.3s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 717.16MB / 853.67MB 11.0s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 422.58MB / 534.48MB 11.0s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 451.94MB / 534.48MB 11.5s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 763.36MB / 853.67MB 12.0s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 484.44MB / 534.48MB 12.2s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 806.35MB / 853.67MB 12.6s\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 513.42MB / 534.48MB 12.7s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 853.67MB / 853.67MB 13.4s\n",
      "#5 extracting sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 10.1s\n",
      "#5 extracting sha256:481786a11c6e3aa39efa375a6b1f74ae01768e4a1c8aaf93d90a6a90f72ae555 11.8s done\n",
      "#5 sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 534.48MB / 534.48MB 16.4s done\n",
      "#5 extracting sha256:275a246fad1f928208d5113b37f73e58516d6acaad2365791187591074dda3ba done\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 853.67MB / 853.67MB 18.5s\n",
      "#5 sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 853.67MB / 853.67MB 18.9s done\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 0.1s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 5.2s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 10.3s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 15.3s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 20.3s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 25.4s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 30.4s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 35.6s\n",
      "#5 extracting sha256:6391690ddce2b8f592301de7080fa07399f3f449f12bcba1c385b359b853a595 39.2s done\n",
      "#5 extracting sha256:bbce0e9b489fb7d2d774c017c9d66b4ddc3f21711b4ab515cd7476ecb23108f9\n",
      "#5 extracting sha256:bbce0e9b489fb7d2d774c017c9d66b4ddc3f21711b4ab515cd7476ecb23108f9 0.2s done\n",
      "#5 extracting sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 0.1s\n",
      "#5 extracting sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 5.2s\n",
      "#5 extracting sha256:a7ec0c7527b6462011b84aefe941cca147ecbad2fd4434cf60c21b47ae6df273 9.8s done\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 5.1s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 10.1s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 15.1s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 20.1s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 25.2s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 30.2s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 36.5s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 41.6s\n",
      "#5 extracting sha256:7aff549e4c97900f245c509324c1764cd1ac098fb0b56759530306b877846cc0 43.2s done\n",
      "#5 extracting sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1\n",
      "#5 extracting sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 done\n",
      "#5 extracting sha256:e4bff5ce614c6a0cd7b3861e686252c65185413921af4563139cafdbf798edcd done\n",
      "#5 extracting sha256:2999545bd1924d977146cd119ab0fafa3fb5848a9a43d87b81c1079adfec3737 done\n",
      "#5 extracting sha256:bd33b5a5ec4a1375956c0d91d660e25482f5d75242e465699d5306d43627c403\n",
      "#5 extracting sha256:bd33b5a5ec4a1375956c0d91d660e25482f5d75242e465699d5306d43627c403 done\n",
      "#5 extracting sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb\n",
      "#5 extracting sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 5.1s\n",
      "#5 extracting sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 10.2s\n",
      "#5 extracting sha256:8fea5fa58717fd75876072fefe9f2d34c58a30a8b7667608ec000a736ce57abb 12.7s done\n",
      "#5 extracting sha256:a73bd9480671f5b4554219bcdc9f743d568dafb25a91037344681cbc4a3a1e80\n",
      "#5 extracting sha256:a73bd9480671f5b4554219bcdc9f743d568dafb25a91037344681cbc4a3a1e80 done\n",
      "#5 extracting sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4\n",
      "#5 extracting sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 5.0s\n",
      "#5 extracting sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 10.4s\n",
      "#5 extracting sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 15.4s\n",
      "#5 extracting sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 20.4s\n",
      "#5 extracting sha256:139c41303dbed517834c4f4fe6019795be16f27a76ab5a0693304c94198cdcb4 24.3s done\n",
      "#5 extracting sha256:d7df78d5fe6174c6b7690274bbcdaae9eba2775740ec950b2757e397b1041ea7\n",
      "#5 extracting sha256:d7df78d5fe6174c6b7690274bbcdaae9eba2775740ec950b2757e397b1041ea7 done\n",
      "#5 extracting sha256:7daa1f38b6ab7e6d39825bf159fdbcf5f8cef0eac8e02c0dc3b05defcba49db7 done\n",
      "#5 extracting sha256:bce9940ec07e9ed16d28173ba85cb9d158516b0cedc387b30a0285063aedc025 done\n",
      "#5 extracting sha256:01f79f64a163d3fa3dcd8d388b0037649ed536eb50e2c3234c46bf4ead66c527\n",
      "#5 extracting sha256:01f79f64a163d3fa3dcd8d388b0037649ed536eb50e2c3234c46bf4ead66c527 done\n",
      "#5 extracting sha256:5113ced62272a5ed4d9dbc29b9d27b24e83c9959a9d4f8dce4debf495cc4a26d done\n",
      "#5 extracting sha256:aa0eae780ea1755c84561d2e50a1f8d79ad5dd51f0b8f5f41287cb249ae3b8ed done\n",
      "#5 extracting sha256:caeb9ac3c4842526e1d3737860f415263105d1a7e8fb2d35bdad31d829aede1a done\n",
      "#5 DONE 156.5s\n",
      "\n",
      "#6 [2/7] RUN mkdir -m 777 -p /usr/app /home\n",
      "#6 DONE 0.4s\n",
      "\n",
      "#7 [3/7] WORKDIR /usr/app\n",
      "#7 DONE 0.0s\n",
      "\n",
      "#8 [4/7] RUN rm -rf /var/sitecustomize\n",
      "#8 DONE 0.4s\n",
      "\n",
      "#9 [5/7] COPY [./setup.py, ./setup.py]\n",
      "#9 DONE 0.0s\n",
      "\n",
      "#10 [6/7] RUN pip3 install --no-cache-dir .\n",
      "#10 1.015 Processing /usr/app\n",
      "#10 1.017   Preparing metadata (setup.py): started\n",
      "#10 1.303   Preparing metadata (setup.py): finished with status 'done'\n",
      "#10 1.308 Building wheels for collected packages: babyweight\n",
      "#10 1.309   Building wheel for babyweight (setup.py): started\n",
      "#10 1.790   Building wheel for babyweight (setup.py): finished with status 'done'\n",
      "#10 1.791   Created wheel for babyweight: filename=babyweight-0.1-py3-none-any.whl size=1014 sha256=8013f499330c953cfe9873c0475fabbf3a1a0d52ca9d34912557fca56a1fd4b5\n",
      "#10 1.792   Stored in directory: /tmp/pip-ephem-wheel-cache-yt5gafk2/wheels/15/e3/ac/a847300672c4b1db4dfdf43328463a2021a4aa40163af1ffc2\n",
      "#10 1.796 Successfully built babyweight\n",
      "#10 3.444 Installing collected packages: babyweight\n",
      "#10 3.495 Successfully installed babyweight-0.1\n",
      "#10 3.495 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "#10 DONE 3.8s\n",
      "\n",
      "#11 [7/7] COPY [trainer, trainer]\n",
      "#11 DONE 0.0s\n",
      "\n",
      "#12 exporting to image\n",
      "#12 exporting layers 0.1s done\n",
      "#12 writing image sha256:3a376c7c050ee81d253f91021cfcc5076dc40b466a4ed0e9ed0073886a1a35dc done\n",
      "#12 naming to gcr.io/qwiklabs-gcp-03-652967eb1d9b/cloudai-autogenerated/babyweight_251101_174255:20251101.17.42.56.905319 done\n",
      "#12 DONE 0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A custom container image is built locally.\n",
      "\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.12/subprocess.py:1010: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.12/subprocess.py:1016: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/qwiklabs-gcp-03-652967eb1d9b/cloudai-autogenerated/babyweight_251101_174255]\n",
      "917c3db455c6: Preparing\n",
      "88c433a8c765: Preparing\n",
      "c8a8f7203097: Preparing\n",
      "d5168c8d6db6: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "b09e19954e86: Preparing\n",
      "e42695c7b436: Preparing\n",
      "e42695c7b436: Preparing\n",
      "7e34967c8575: Preparing\n",
      "03aa2a4bdb68: Preparing\n",
      "69ff3552dab2: Preparing\n",
      "bde9e2053036: Preparing\n",
      "bde9e2053036: Preparing\n",
      "b253aec57174: Preparing\n",
      "e9a5c35692b6: Preparing\n",
      "5ca5a09f80b2: Preparing\n",
      "f27306b95858: Preparing\n",
      "e96984247094: Preparing\n",
      "bf89224ff876: Preparing\n",
      "ca7739d6661c: Preparing\n",
      "ca7739d6661c: Preparing\n",
      "6afff9338181: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "380cd88b9fb2: Preparing\n",
      "25c9ddea4aaa: Preparing\n",
      "eec152ec24b8: Preparing\n",
      "dd7d6ac03700: Preparing\n",
      "be9dc4e2456b: Preparing\n",
      "ceab7f116eb5: Preparing\n",
      "bd5ff18df433: Preparing\n",
      "a27f4aa3db94: Preparing\n",
      "1a102d1cac2b: Preparing\n",
      "b09e19954e86: Waiting\n",
      "e42695c7b436: Waiting\n",
      "7e34967c8575: Waiting\n",
      "03aa2a4bdb68: Waiting\n",
      "69ff3552dab2: Waiting\n",
      "bde9e2053036: Waiting\n",
      "b253aec57174: Waiting\n",
      "e9a5c35692b6: Waiting\n",
      "5ca5a09f80b2: Waiting\n",
      "f27306b95858: Waiting\n",
      "e96984247094: Waiting\n",
      "bf89224ff876: Waiting\n",
      "ca7739d6661c: Waiting\n",
      "6afff9338181: Waiting\n",
      "380cd88b9fb2: Waiting\n",
      "25c9ddea4aaa: Waiting\n",
      "eec152ec24b8: Waiting\n",
      "dd7d6ac03700: Waiting\n",
      "be9dc4e2456b: Waiting\n",
      "ceab7f116eb5: Waiting\n",
      "bd5ff18df433: Waiting\n",
      "a27f4aa3db94: Waiting\n",
      "1a102d1cac2b: Waiting\n",
      "5f70bf18a086: Layer already exists\n",
      "917c3db455c6: Pushed\n",
      "d5168c8d6db6: Pushed\n",
      "c8a8f7203097: Pushed\n",
      "88c433a8c765: Pushed\n",
      "b09e19954e86: Pushed\n",
      "7e34967c8575: Pushed\n",
      "e42695c7b436: Pushed\n",
      "03aa2a4bdb68: Pushed\n",
      "69ff3552dab2: Pushed\n",
      "bde9e2053036: Pushed\n",
      "b253aec57174: Pushed\n",
      "e9a5c35692b6: Pushed\n",
      "f27306b95858: Pushed\n",
      "ca7739d6661c: Pushed\n",
      "6afff9338181: Pushed\n",
      "bf89224ff876: Pushed\n",
      "eec152ec24b8: Pushed\n",
      "25c9ddea4aaa: Pushed\n",
      "be9dc4e2456b: Pushed\n",
      "ceab7f116eb5: Pushed\n",
      "e96984247094: Pushed\n",
      "bd5ff18df433: Pushed\n",
      "a27f4aa3db94: Pushed\n",
      "1a102d1cac2b: Layer already exists\n",
      "dd7d6ac03700: Pushed\n",
      "5ca5a09f80b2: Pushed\n",
      "380cd88b9fb2: Pushed\n",
      "20251101.17.42.56.905319: digest: sha256:ae8cbb3755a68c23f641dbe5060ba5b741df1a0158193c55958b2eea87615a59 size: 6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom container image [gcr.io/qwiklabs-gcp-03-652967eb1d9b/cloudai-autogenerated/babyweight_251101_174255:20251101.17.42.56.905319] is created for your custom job.\n",
      "\n",
      "CustomJob [projects/1085470363887/locations/us-central1/customJobs/3753837099292295168] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/1085470363887/locations/us-central1/customJobs/3753837099292295168\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/1085470363887/locations/us-central1/customJobs/3753837099292295168\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=${JOBNAME} \\\n",
    "  --worker-pool-spec=machine-type=n1-standard-8,executor-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest,local-package-path=babyweight/,python-module=trainer.task \\\n",
    "  --args=\"--train_data_path=gs://${BUCKET}/babyweight/data/train*.csv\",\"--eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv\",\"--output_dir=${OUTDIR}\",\"--num_epochs=10\",\"--train_examples=10000\",\"--eval_steps=100\",\"--batch_size=32\",\"--nembeds=8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2025-11-01 17:50:59 +0000\tservice\tWaiting for job to be provisioned.\n",
      "INFO\t2025-11-01 17:50:59 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:04 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:09 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:14 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:19 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:24 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:29 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:34 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:39 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:44 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:49 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:54 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:51:59 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:04 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:09 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:14 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:19 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:24 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:29 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:34 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:39 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:44 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:49 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:54 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:52:59 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:04 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:09 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:14 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:19 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:24 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:29 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:34 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:40 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:45 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:50 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:53:55 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:00 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:05 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:10 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:15 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:20 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:25 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:30 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:35 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:40 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:45 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:50 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:54:55 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:00 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:05 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:10 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:15 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:20 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:25 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:30 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:35 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:40 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:45 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:50 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:55:55 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:00 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:05 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:10 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:15 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:20 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:25 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:30 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:35 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:40 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:45 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:50 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:56:55 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:57:00 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:57:05 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:57:10 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:57:15 +0000\tservice\tVertex AI is provisioning job running framework. First time usage might take couple of minutes, and subsequent runs can be much faster.\n",
      "INFO\t2025-11-01 17:57:15 +0000\tservice\tVertex AI is setting up this job.\n",
      "INFO\t2025-11-01 17:57:15 +0000\tservice\tWaiting for training program to start.\n",
      "INFO\t2025-11-01 17:57:16 +0000\tservice\tJob is preparing.\n",
      "INFO\t2025-11-01 18:00:29 +0000\tservice\tJob is running.\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:00:31.498605: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:00:33.486939: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:00:33.486986: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:00:46.691908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:00:46.694062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:00:46.694102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:01:04.847967: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:01:04.848018: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:01:04.848058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gk3-cml-1101-175059-be82-nap-bho9b4th-a08ebcd7-h7hv): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t2025-11-01 18:01:04.865223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "ERROR\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tModel: \"model\"\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t__________________________________________________________________________________________________\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t==================================================================================================\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t gestation_weeks (InputLayer)   [(None,)]            0           []                               \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t is_male (InputLayer)           [(None,)]            0           []                               \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t mother_age (InputLayer)        [(None,)]            0           []                               \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t plurality (InputLayer)         [(None,)]            0           []                               \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t deep_inputs (DenseFeatures)    (None, 10)           8000        ['gestation_weeks[0][0]',        \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                  'is_male[0][0]',                \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                  'mother_age[0][0]',             \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                  'plurality[0][0]']              \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t dnn_1 (Dense)                  (None, 128)          1408        ['deep_inputs[0][0]']            \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t dnn_2 (Dense)                  (None, 32)           4128        ['dnn_1[0][0]']                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t wide_inputs (DenseFeatures)    (None, 71)           0           ['gestation_weeks[0][0]',        \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                  'is_male[0][0]',                \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                  'mother_age[0][0]',             \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                  'plurality[0][0]']              \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t dnn_3 (Dense)                  (None, 4)            132         ['dnn_2[0][0]']                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t linear (Dense)                 (None, 10)           720         ['wide_inputs[0][0]']            \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t both (Concatenate)             (None, 14)           0           ['dnn_3[0][0]',                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                  'linear[0][0]']                 \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t weight (Dense)                 (None, 1)            15          ['both[0][0]']                   \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t                                                                                                  \n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t==================================================================================================\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tTotal params: 14,403\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tTrainable params: 14,403\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tNon-trainable params: 0\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\t__________________________________________________________________________________________________\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tNone\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tmode = train\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tmode = eval\n",
      "INFO\t2025-11-01 18:01:32 +0000\tworkerpool0-0\tEpoch 1/10\n",
      "INFO\t2025-11-01 18:02:28 +0000\tworkerpool0-0\t\n",
      "INFO\t2025-11-01 18:02:28 +0000\tworkerpool0-0\tEpoch 1: saving model to gs://testbucket1_unique/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2025-11-01 18:02:32 +0000\tworkerpool0-0\t31250/31250 - 86s - loss: 1.0696 - rmse: 0.9879 - mse: 1.0696 - val_loss: 1.2377 - val_rmse: 1.0993 - val_mse: 1.2377 - 86s/epoch - 3ms/step\n",
      "INFO\t2025-11-01 18:02:32 +0000\tworkerpool0-0\tEpoch 2/10\n",
      "INFO\t2025-11-01 18:03:36 +0000\tworkerpool0-0\t\n",
      "INFO\t2025-11-01 18:03:36 +0000\tworkerpool0-0\tEpoch 2: saving model to gs://testbucket1_unique/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2025-11-01 18:03:40 +0000\tworkerpool0-0\t31250/31250 - 68s - loss: 1.0235 - rmse: 0.9830 - mse: 1.0235 - val_loss: 1.2851 - val_rmse: 1.1214 - val_mse: 1.2851 - 68s/epoch - 2ms/step\n",
      "INFO\t2025-11-01 18:03:40 +0000\tworkerpool0-0\tEpoch 3/10\n",
      "INFO\t2025-11-01 18:04:44 +0000\tworkerpool0-0\t\n",
      "INFO\t2025-11-01 18:04:44 +0000\tworkerpool0-0\tEpoch 3: saving model to gs://testbucket1_unique/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2025-11-01 18:04:47 +0000\tworkerpool0-0\t31250/31250 - 68s - loss: 1.0052 - rmse: 0.9713 - mse: 1.0052 - val_loss: 1.5177 - val_rmse: 1.2176 - val_mse: 1.5177 - 68s/epoch - 2ms/step\n",
      "INFO\t2025-11-01 18:04:47 +0000\tworkerpool0-0\tEpoch 4/10\n",
      "INFO\t2025-11-01 18:05:51 +0000\tworkerpool0-0\t\n",
      "INFO\t2025-11-01 18:05:51 +0000\tworkerpool0-0\tEpoch 4: saving model to gs://testbucket1_unique/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2025-11-01 18:05:55 +0000\tworkerpool0-0\t31250/31250 - 68s - loss: 1.0160 - rmse: 0.9791 - mse: 1.0160 - val_loss: 1.4272 - val_rmse: 1.1805 - val_mse: 1.4272 - 68s/epoch - 2ms/step\n",
      "INFO\t2025-11-01 18:05:55 +0000\tworkerpool0-0\tEpoch 5/10\n",
      "INFO\t2025-11-01 18:07:00 +0000\tworkerpool0-0\t\n",
      "INFO\t2025-11-01 18:07:00 +0000\tworkerpool0-0\tEpoch 5: saving model to gs://testbucket1_unique/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2025-11-01 18:07:03 +0000\tworkerpool0-0\t31250/31250 - 68s - loss: 1.0134 - rmse: 0.9779 - mse: 1.0134 - val_loss: 1.0876 - val_rmse: 1.0265 - val_mse: 1.0876 - 68s/epoch - 2ms/step\n",
      "INFO\t2025-11-01 18:07:03 +0000\tworkerpool0-0\tEpoch 6/10\n",
      "INFO\t2025-11-01 18:08:08 +0000\tworkerpool0-0\t\n",
      "INFO\t2025-11-01 18:08:08 +0000\tworkerpool0-0\tEpoch 6: saving model to gs://testbucket1_unique/babyweight/trained_model/checkpoints/babyweight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process was interrupted.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'gcloud ai custom-jobs stream-logs projects/1085470363887/locations/us-central1/customJobs/3753837099292295168\\n'' died with <Signals.SIGINT: 2>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgcloud ai custom-jobs stream-logs projects/1085470363887/locations/us-central1/customJobs/3753837099292295168\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:159\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:327\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while terminating subprocess (pid=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mpid, e))\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(p\u001b[38;5;241m.\u001b[39mreturncode, cell) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'gcloud ai custom-jobs stream-logs projects/1085470363887/locations/us-central1/customJobs/3753837099292295168\\n'' died with <Signals.SIGINT: 2>."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai custom-jobs stream-logs projects/1085470363887/locations/us-central1/customJobs/3753837099292295168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job should complete within 15 to 20 minutes. You do not need to wait for this training job to finish before moving forward in the notebook, but will need a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2025-11-01T17:48:22.686381Z'\n",
      "displayName: babyweight_251101_174255\n",
      "endTime: '2025-11-01T18:13:04Z'\n",
      "jobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - containerSpec:\n",
      "      args:\n",
      "      - --train_data_path=gs://testbucket1_unique/babyweight/data/train*.csv\n",
      "      - --eval_data_path=gs://testbucket1_unique/babyweight/data/eval*.csv\n",
      "      - --output_dir=gs://testbucket1_unique/babyweight/trained_model\n",
      "      - --num_epochs=10\n",
      "      - --train_examples=10000\n",
      "      - --eval_steps=100\n",
      "      - --batch_size=32\n",
      "      - --nembeds=8\n",
      "      imageUri: gcr.io/qwiklabs-gcp-03-652967eb1d9b/cloudai-autogenerated/babyweight_251101_174255:20251101.17.42.56.905319\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      machineType: n1-standard-8\n",
      "    replicaCount: '1'\n",
      "name: projects/1085470363887/locations/us-central1/customJobs/3753837099292295168\n",
      "startTime: '2025-11-01T18:00:29Z'\n",
      "state: JOB_STATE_SUCCEEDED\n",
      "updateTime: '2025-11-01T18:13:28.450541Z'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai custom-jobs describe projects/1085470363887/locations/us-central1/customJobs/3753837099292295168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our trained model files\n",
    "\n",
    "Let's check the directory structure of our outputs of our trained model in folder we exported. We'll want to deploy the saved_model.pb within the timestamped directory as well as the variable values in the variables folder. Therefore, we need the path of the timestamped directory so that everything within it can be found by Cloud AI Platform's model deployment service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://testbucket1_unique/babyweight/trained_model/\n",
      "gs://testbucket1_unique/babyweight/trained_model/20251101181247/\n",
      "gs://testbucket1_unique/babyweight/trained_model/checkpoints/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/trained_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://testbucket1_unique/babyweight/trained_model/20251101181247/\n",
      "gs://testbucket1_unique/babyweight/trained_model/20251101181247/fingerprint.pb\n",
      "gs://testbucket1_unique/babyweight/trained_model/20251101181247/saved_model.pb\n",
      "gs://testbucket1_unique/babyweight/trained_model/20251101181247/assets/\n",
      "gs://testbucket1_unique/babyweight/trained_model/20251101181247/variables/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(gsutil ls -ld -- gs://${BUCKET}/babyweight/trained_model/2* \\\n",
    "                 | tail -1)\n",
    "gsutil ls ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy trained model\n",
    "\n",
    "Deploying the trained model to act as a REST web service is a simple gcloud call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [4224736763869921280]...\n",
      ".....................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"babyweight\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_DIR=$(gsutil ls -d gs://${BUCKET}/babyweight/trained_model/2* | tail -1)\n",
    "\n",
    "# Create a model resource in Vertex AI\n",
    "gcloud ai models upload \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=${MODEL_NAME} \\\n",
    "  --artifact-uri=${MODEL_DIR} \\\n",
    "  --container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [8194659835397013504]...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ENDPOINT_NAME=\"babyweight-endpoint\"\n",
    "\n",
    "# Create endpoint\n",
    "gcloud ai endpoints create --region=${REGION} --display-name=${ENDPOINT_NAME}\n",
    "\n",
    "# Deploy model\n",
    "gcloud ai endpoints deploy-model ENDPOINT_ID \\\n",
    "  --region=${REGION} \\\n",
    "  --model=MODEL_ID \\\n",
    "  --machine-type=n1-standard-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
